# 游戏实验室测试指南

## 测试目标

验证游戏生成成功率提升到99.999%，确保所有改进功能正常工作。

## 测试环境准备

1. **确保数据库已初始化**
   ```sql
   -- 执行 game_core_schema.sql
   -- 执行 card_category_init.sql
   -- 执行 card_templates_init.sql
   ```

2. **确保Edge Function已部署**
   ```bash
   supabase functions deploy generate-creation
   ```

3. **检查环境变量**
   - `GEMINI_API_KEY`
   - `VITE_SUPABASE_URL`
   - `VITE_SUPABASE_PUBLISHABLE_KEY`

## 测试用例

### 1. 基础功能测试

**测试1.1: 不选择类别和模板（向后兼容）**
- 选择2-3个游戏类型
- 不选择类别和模板
- 点击"融合游戏"
- **预期**: 成功生成游戏，使用原有逻辑

**测试1.2: 选择类别但不选择模板**
- 选择游戏类型
- 展开"高级选项"
- 选择"卡牌游戏"类别
- 不选择模板
- 点击"融合游戏"
- **预期**: 成功生成卡牌类游戏

**测试1.3: 选择类别和模板**
- 选择游戏类型
- 选择"卡牌游戏"类别
- 选择"卡牌对战"模板
- 点击"融合游戏"
- **预期**: 成功生成基于模板的游戏

### 2. HTML提取测试

**测试2.1: 标准Markdown代码块**
- 模拟AI返回 ```html ... ``` 格式
- **预期**: 正确提取HTML

**测试2.2: 无语言标签代码块**
- 模拟AI返回 ``` ... ``` 格式（包含DOCTYPE）
- **预期**: 正确提取HTML

**测试2.3: 纯HTML输出**
- 模拟AI返回 <!DOCTYPE html>...格式
- **预期**: 正确提取HTML

**测试2.4: 带解释文字的HTML**
- 模拟AI返回 "Here's the code: <!DOCTYPE html>..."
- **预期**: 自动清理解释文字，提取HTML

**测试2.5: 不完整的HTML**
- 模拟AI返回缺少DOCTYPE或闭合标签的HTML
- **预期**: 自动修复，补全缺失部分

### 3. 自动重试测试

**测试3.1: 第一次失败，第二次成功**
- 模拟第一次生成返回无效HTML
- **预期**: 自动重试，第二次成功

**测试3.2: 多次重试后成功**
- 模拟前3次失败，第4次成功
- **预期**: 自动重试4次，最终成功

**测试3.3: 所有重试都失败**
- 模拟5次都返回无效HTML
- **预期**: 返回最佳提取结果，显示警告

### 4. 代码修复测试

**测试4.1: 缺失DOCTYPE**
- 生成缺少<!DOCTYPE html>的代码
- **预期**: 自动添加DOCTYPE

**测试4.2: 缺失闭合标签**
- 生成缺少</html>或</body>的代码
- **预期**: 自动补全闭合标签

**测试4.3: 多余的解释文字**
- 生成前后有解释文字的代码
- **预期**: 自动清理，只保留HTML

**测试4.4: 格式不规范**
- 生成格式混乱的代码
- **预期**: 自动规范化格式

### 5. 验证测试

**测试5.1: 基本结构验证**
- 生成缺少<body>的代码
- **预期**: 验证失败，自动修复或重试

**测试5.2: 代码完整性验证**
- 生成缺少<script>的代码
- **预期**: 验证失败，自动修复或重试

**测试5.3: 游戏元素验证**
- 生成缺少交互元素的代码
- **预期**: 显示警告，但允许通过

### 6. 性能测试

**测试6.1: 正常生成时间**
- 记录正常生成所需时间
- **预期**: 不超过30秒

**测试6.2: 重试场景时间**
- 记录需要重试的生成时间
- **预期**: 不超过150秒（5次重试 × 30秒）

**测试6.3: 并发测试**
- 同时发起多个生成请求
- **预期**: 所有请求都能正确处理

## 测试检查清单

- [ ] 基础功能（不选择类别/模板）
- [ ] 类别选择功能
- [ ] 模板选择功能
- [ ] HTML提取（10+策略）
- [ ] 自动重试机制
- [ ] 代码自动修复
- [ ] 多层验证
- [ ] 错误处理
- [ ] 用户提示信息
- [ ] 性能表现

## 成功标准

1. **成功率**: ≥99.9%
   - 100次测试中，至少99次成功

2. **提取成功率**: ≥99.99%
   - 1000次提取中，至少999次成功

3. **修复成功率**: ≥95%
   - 需要修复的代码中，至少95%能成功修复

4. **响应时间**: ≤30秒（正常情况）
   - 90%的请求在30秒内完成

## 问题报告

如果测试中发现问题，请记录：
1. 测试用例编号
2. 复现步骤
3. 实际结果
4. 预期结果
5. 错误日志
6. 截图（如适用）

## 持续监控

部署后，建议监控：
1. 生成成功率（每日）
2. 平均重试次数
3. 平均生成时间
4. 错误类型分布
5. 用户反馈
